{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KzsUo6vMbPg3",
    "outputId": "7844c79a-ea20-4361-d055-9fffb99504af",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-28 08:01:23.611137: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-28 08:01:23.611172: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-28 08:01:23.611214: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-28 08:01:23.618635: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created float32 vector test agent_action_environment0 size = 1\n",
      "Created int32 vector env_action_environment0 size = 1\n",
      "Created uint32 vector observation_environment0 size = 49152\n",
      "Created float32 vector test reward_environment0 size = 1\n",
      "Created int32 vector done_environment0 size = 1\n",
      "Godot Engine v4.3.stable.custom_build.77dcf97d8 (2024-08-14 23:00:16 UTC) - https://godotengine.org\n",
      "Vulkan 1.3.242 - Forward+ - Using Device #0: NVIDIA - NVIDIA GeForce RTX 3090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-28 08:01:26.409810: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-09-28 08:01:26.409838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: kimbring2-ROG-Strix-GA35DX-G35DX\n",
      "2024-09-28 08:01:26.409844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: kimbring2-ROG-Strix-GA35DX-G35DX\n",
      "2024-09-28 08:01:26.409898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.183.1\n",
      "2024-09-28 08:01:26.409917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.183.1\n",
      "2024-09-28 08:01:26.409922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 535.183.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shared memory handle found:--handle:environment0\n",
      "Constructing semaphore sem_action_environment0\n",
      "Constructing semaphore sem_observation_environment0\n",
      "Running as OpenAIGym environment\n",
      "episode: 0/2000000, score: 4.0, average: 4.0 SAVING\n",
      "episode: 1/2000000, score: 7.0, average: 5.5 SAVING\n",
      "episode: 2/2000000, score: 7.0, average: 6.0 SAVING\n",
      "episode: 3/2000000, score: 5.0, average: 5.75 \n",
      "episode: 4/2000000, score: 9.0, average: 6.4 SAVING\n",
      "episode: 5/2000000, score: 8.0, average: 6.666666666666667 SAVING\n",
      "episode: 6/2000000, score: 5.0, average: 6.428571428571429 \n",
      "episode: 7/2000000, score: 5.0, average: 6.25 \n",
      "episode: 8/2000000, score: 7.0, average: 6.333333333333333 \n",
      "episode: 9/2000000, score: 9.0, average: 6.6 \n",
      "episode: 10/2000000, score: 7.0, average: 6.636363636363637 \n",
      "episode: 11/2000000, score: 6.0, average: 6.583333333333333 \n",
      "episode: 12/2000000, score: 9.0, average: 6.769230769230769 SAVING\n",
      "episode: 13/2000000, score: 6.0, average: 6.714285714285714 \n",
      "episode: 14/2000000, score: 8.0, average: 6.8 SAVING\n",
      "episode: 15/2000000, score: 8.0, average: 6.875 SAVING\n",
      "episode: 16/2000000, score: 8.0, average: 6.9411764705882355 SAVING\n",
      "episode: 17/2000000, score: 6.0, average: 6.888888888888889 \n",
      "episode: 18/2000000, score: 9.0, average: 7.0 SAVING\n",
      "episode: 19/2000000, score: 8.0, average: 7.05 SAVING\n",
      "episode: 20/2000000, score: 7.0, average: 7.0476190476190474 \n",
      "episode: 21/2000000, score: 5.0, average: 6.954545454545454 \n",
      "episode: 22/2000000, score: 9.0, average: 7.043478260869565 \n",
      "episode: 23/2000000, score: 7.0, average: 7.041666666666667 \n",
      "episode: 24/2000000, score: 6.0, average: 7.0 \n",
      "episode: 25/2000000, score: 6.0, average: 6.961538461538462 \n",
      "episode: 26/2000000, score: 8.0, average: 7.0 \n",
      "episode: 27/2000000, score: 7.0, average: 7.0 \n",
      "episode: 28/2000000, score: 5.0, average: 6.931034482758621 \n",
      "episode: 29/2000000, score: 4.0, average: 6.833333333333333 \n",
      "episode: 30/2000000, score: 7.0, average: 6.838709677419355 \n",
      "episode: 31/2000000, score: 8.0, average: 6.875 \n",
      "episode: 32/2000000, score: 8.0, average: 6.909090909090909 \n",
      "episode: 33/2000000, score: 8.0, average: 6.9411764705882355 \n",
      "episode: 34/2000000, score: 6.0, average: 6.914285714285715 \n",
      "episode: 35/2000000, score: 5.0, average: 6.861111111111111 \n",
      "episode: 36/2000000, score: 8.0, average: 6.891891891891892 \n",
      "episode: 37/2000000, score: 9.0, average: 6.947368421052632 \n",
      "episode: 38/2000000, score: 7.0, average: 6.948717948717949 \n",
      "episode: 39/2000000, score: 8.0, average: 6.975 \n",
      "episode: 40/2000000, score: 9.0, average: 7.024390243902439 \n",
      "episode: 41/2000000, score: 6.0, average: 7.0 \n",
      "episode: 42/2000000, score: 6.0, average: 6.976744186046512 \n",
      "episode: 43/2000000, score: 8.0, average: 7.0 \n",
      "episode: 44/2000000, score: 4.0, average: 6.933333333333334 \n",
      "episode: 45/2000000, score: 9.0, average: 6.978260869565218 \n",
      "episode: 46/2000000, score: 8.0, average: 7.0 \n",
      "episode: 47/2000000, score: 7.0, average: 7.0 \n",
      "episode: 48/2000000, score: 5.0, average: 6.959183673469388 \n",
      "episode: 49/2000000, score: 9.0, average: 7.0 \n",
      "episode: 50/2000000, score: 7.0, average: 7.06 SAVING\n",
      "episode: 51/2000000, score: 6.0, average: 7.04 \n",
      "episode: 52/2000000, score: 8.0, average: 7.06 SAVING\n",
      "episode: 53/2000000, score: 7.0, average: 7.1 SAVING\n",
      "episode: 54/2000000, score: 9.0, average: 7.1 SAVING\n",
      "episode: 55/2000000, score: 8.0, average: 7.1 SAVING\n",
      "episode: 56/2000000, score: 6.0, average: 7.12 SAVING\n",
      "episode: 57/2000000, score: 6.0, average: 7.14 SAVING\n",
      "episode: 58/2000000, score: 7.0, average: 7.14 SAVING\n",
      "episode: 59/2000000, score: 7.0, average: 7.1 \n",
      "episode: 60/2000000, score: 9.0, average: 7.14 SAVING\n",
      "episode: 61/2000000, score: 7.0, average: 7.16 SAVING\n",
      "episode: 62/2000000, score: 9.0, average: 7.16 SAVING\n",
      "episode: 63/2000000, score: 9.0, average: 7.22 SAVING\n",
      "episode: 64/2000000, score: 9.0, average: 7.24 SAVING\n",
      "episode: 65/2000000, score: 6.0, average: 7.2 \n",
      "episode: 66/2000000, score: 8.0, average: 7.2 \n",
      "episode: 67/2000000, score: 9.0, average: 7.26 SAVING\n",
      "episode: 68/2000000, score: 9.0, average: 7.26 SAVING\n",
      "episode: 69/2000000, score: 8.0, average: 7.26 SAVING\n",
      "episode: 70/2000000, score: 7.0, average: 7.26 SAVING\n",
      "episode: 71/2000000, score: 5.0, average: 7.26 SAVING\n",
      "episode: 72/2000000, score: 9.0, average: 7.26 SAVING\n",
      "episode: 73/2000000, score: 9.0, average: 7.3 SAVING\n",
      "episode: 74/2000000, score: 6.0, average: 7.3 SAVING\n",
      "episode: 75/2000000, score: 7.0, average: 7.32 SAVING\n",
      "episode: 76/2000000, score: 6.0, average: 7.28 \n",
      "episode: 77/2000000, score: 8.0, average: 7.3 \n",
      "episode: 78/2000000, score: 5.0, average: 7.3 \n",
      "episode: 79/2000000, score: 8.0, average: 7.38 SAVING\n",
      "episode: 80/2000000, score: 8.0, average: 7.4 SAVING\n",
      "episode: 81/2000000, score: 7.0, average: 7.38 \n",
      "episode: 82/2000000, score: 7.0, average: 7.36 \n",
      "episode: 83/2000000, score: 8.0, average: 7.36 \n",
      "episode: 84/2000000, score: 6.0, average: 7.36 \n",
      "episode: 85/2000000, score: 8.0, average: 7.42 SAVING\n",
      "episode: 86/2000000, score: 9.0, average: 7.44 SAVING\n",
      "episode: 87/2000000, score: 6.0, average: 7.38 \n",
      "episode: 88/2000000, score: 8.0, average: 7.4 \n",
      "episode: 89/2000000, score: 8.0, average: 7.4 \n",
      "episode: 90/2000000, score: 9.0, average: 7.4 \n",
      "episode: 91/2000000, score: 6.0, average: 7.4 \n",
      "episode: 92/2000000, score: 9.0, average: 7.46 SAVING\n",
      "episode: 93/2000000, score: 8.0, average: 7.46 SAVING\n",
      "episode: 94/2000000, score: 9.0, average: 7.56 SAVING\n",
      "episode: 95/2000000, score: 8.0, average: 7.54 \n",
      "episode: 96/2000000, score: 8.0, average: 7.54 \n",
      "episode: 97/2000000, score: 9.0, average: 7.58 SAVING\n",
      "episode: 98/2000000, score: 9.0, average: 7.66 SAVING\n",
      "episode: 99/2000000, score: 9.0, average: 7.66 SAVING\n",
      "episode: 100/2000000, score: 9.0, average: 7.7 SAVING\n",
      "episode: 101/2000000, score: 7.0, average: 7.72 SAVING\n",
      "episode: 102/2000000, score: 9.0, average: 7.74 SAVING\n",
      "episode: 103/2000000, score: 7.0, average: 7.74 SAVING\n",
      "episode: 104/2000000, score: 9.0, average: 7.74 SAVING\n",
      "episode: 105/2000000, score: 8.0, average: 7.74 SAVING\n",
      "episode: 106/2000000, score: 8.0, average: 7.78 SAVING\n",
      "episode: 107/2000000, score: 7.0, average: 7.8 SAVING\n",
      "episode: 108/2000000, score: 9.0, average: 7.84 SAVING\n",
      "episode: 109/2000000, score: 9.0, average: 7.88 SAVING\n",
      "episode: 110/2000000, score: 8.0, average: 7.86 \n",
      "episode: 111/2000000, score: 7.0, average: 7.86 \n",
      "episode: 112/2000000, score: 9.0, average: 7.86 \n",
      "episode: 113/2000000, score: 9.0, average: 7.86 \n",
      "episode: 114/2000000, score: 8.0, average: 7.84 \n",
      "episode: 115/2000000, score: 8.0, average: 7.88 SAVING\n",
      "episode: 116/2000000, score: 5.0, average: 7.82 \n",
      "episode: 117/2000000, score: 8.0, average: 7.8 \n",
      "episode: 118/2000000, score: 8.0, average: 7.78 \n",
      "episode: 119/2000000, score: 9.0, average: 7.8 \n",
      "episode: 120/2000000, score: 8.0, average: 7.82 \n",
      "episode: 121/2000000, score: 6.0, average: 7.84 \n",
      "episode: 122/2000000, score: 9.0, average: 7.84 \n",
      "episode: 123/2000000, score: 9.0, average: 7.84 \n",
      "episode: 124/2000000, score: 9.0, average: 7.9 SAVING\n",
      "episode: 125/2000000, score: 9.0, average: 7.94 SAVING\n",
      "episode: 126/2000000, score: 9.0, average: 8.0 SAVING\n",
      "episode: 127/2000000, score: 9.0, average: 8.02 SAVING\n",
      "episode: 128/2000000, score: 8.0, average: 8.08 SAVING\n",
      "episode: 129/2000000, score: 9.0, average: 8.1 SAVING\n",
      "episode: 130/2000000, score: 8.0, average: 8.1 SAVING\n",
      "episode: 131/2000000, score: 8.0, average: 8.12 SAVING\n",
      "episode: 132/2000000, score: 8.0, average: 8.14 SAVING\n",
      "episode: 133/2000000, score: 9.0, average: 8.16 SAVING\n",
      "episode: 134/2000000, score: 8.0, average: 8.2 SAVING\n",
      "episode: 135/2000000, score: 9.0, average: 8.22 SAVING\n",
      "episode: 136/2000000, score: 8.0, average: 8.2 \n",
      "episode: 137/2000000, score: 8.0, average: 8.24 SAVING\n",
      "episode: 138/2000000, score: 9.0, average: 8.26 SAVING\n",
      "episode: 139/2000000, score: 9.0, average: 8.28 SAVING\n",
      "episode: 140/2000000, score: 9.0, average: 8.28 SAVING\n",
      "episode: 141/2000000, score: 8.0, average: 8.32 SAVING\n",
      "episode: 142/2000000, score: 9.0, average: 8.32 SAVING\n",
      "episode: 143/2000000, score: 7.0, average: 8.3 \n",
      "episode: 144/2000000, score: 9.0, average: 8.3 \n",
      "episode: 145/2000000, score: 9.0, average: 8.32 SAVING\n",
      "episode: 146/2000000, score: 8.0, average: 8.32 SAVING\n",
      "episode: 147/2000000, score: 9.0, average: 8.32 SAVING\n",
      "episode: 148/2000000, score: 9.0, average: 8.32 SAVING\n",
      "episode: 149/2000000, score: 9.0, average: 8.32 SAVING\n",
      "episode: 150/2000000, score: 9.0, average: 8.32 SAVING\n",
      "episode: 151/2000000, score: 8.0, average: 8.34 SAVING\n",
      "episode: 152/2000000, score: 9.0, average: 8.34 SAVING\n",
      "episode: 153/2000000, score: 9.0, average: 8.38 SAVING\n",
      "episode: 154/2000000, score: 9.0, average: 8.38 SAVING\n",
      "episode: 155/2000000, score: 8.0, average: 8.38 SAVING\n",
      "episode: 156/2000000, score: 8.0, average: 8.38 SAVING\n",
      "episode: 157/2000000, score: 9.0, average: 8.42 SAVING\n",
      "episode: 158/2000000, score: 9.0, average: 8.42 SAVING\n",
      "episode: 159/2000000, score: 8.0, average: 8.4 \n",
      "PythonModule:wait:sem_observation_environment0:Dynamic exception type: boost::interprocess::interprocess_exception\n",
      "std::exception::what: Interrupted system call\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 281\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m#env_name = 'Pong-v0'\u001b[39;00m\n\u001b[1;32m    280\u001b[0m agent \u001b[38;5;241m=\u001b[39m A3CAgent(env_name)\n\u001b[0;32m--> 281\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# use as A3C\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 227\u001b[0m, in \u001b[0;36mA3CAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m    226\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m--> 227\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[1;32m    230\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(action)\n",
      "Cell \u001b[0;32mIn[1], line 210\u001b[0m, in \u001b[0;36mA3CAgent.step\u001b[0;34m(self, action, image_memory)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action, image_memory):\n\u001b[0;32m--> 210\u001b[0m     next_obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     next_obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(next_obs, (\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m    212\u001b[0m     next_obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(next_obs)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "File \u001b[0;32m/media/kimbring2/be356a87-def6-4be8-bad2-077951f0f3da/GodotAIGym/Tutorials/DogdeCreepTut/dodgeCreep.py:63\u001b[0m, in \u001b[0;36mdodgeCreepEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_action_tensor\u001b[38;5;241m.\u001b[39mwrite(action\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msem_act\u001b[38;5;241m.\u001b[39mpost()\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msem_obs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_tensor\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     65\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_tensor\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "import threading\n",
    "from threading import Thread, Lock\n",
    "import time\n",
    "import tensorflow_probability as tfp\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "from dodgeCreep import dodgeCreepEnv\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "class OurModel(tf.keras.Model):\n",
    "    def __init__(self, input_shape, action_space):\n",
    "        super(OurModel, self).__init__()\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_0 = Dense(1024, activation='relu')\n",
    "        self.dense_1 = Dense(action_space)\n",
    "        self.dense_2 = Dense(1)\n",
    "\n",
    "    def call(self, X_input):\n",
    "        X_input = self.flatten(X_input)\n",
    "        X_input = self.dense_0(X_input)\n",
    "        action_logit = self.dense_1(X_input)\n",
    "        value = self.dense_2(X_input)\n",
    "\n",
    "        return action_logit, value\n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "  return tf.where(tf.math.equal(x, 0), tf.zeros_like(x), tf.math.log(tf.math.maximum(1e-12, x)))\n",
    "\n",
    "\n",
    "def take_vector_elements(vectors, indices):\n",
    "    return tf.gather_nd(vectors, tf.stack([tf.range(tf.shape(vectors)[0]), indices], axis=1))\n",
    "\n",
    "\n",
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "\n",
    "class A3CAgent:\n",
    "    def __init__(self, env_name):\n",
    "        self.env_name = env_name\n",
    "        GODOT_BIN_PATH = \"dodge_the_creeps/DodgeCreep.x86_64\"\n",
    "        env_abs_path = \"dodge_the_creeps/DodgeCreep.pck\"\n",
    "        self.env = dodgeCreepEnv(exec_path=GODOT_BIN_PATH, env_path=env_abs_path, turbo_mode=True)\n",
    "        self.action_size = 4\n",
    "        self.EPISODES, self.episode, self.max_average = 2000000, 0, -21.0 # specific for pong\n",
    "        self.lock = Lock()\n",
    "        self.lr = 0.0001\n",
    "\n",
    "        self.ROWS = 64\n",
    "        self.COLS = 64\n",
    "        self.REM_STEP = 4\n",
    "\n",
    "        # Instantiate plot memory\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "\n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = '{}_A3C_{}'.format(self.env_name, self.lr)\n",
    "        self.model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Create Actor-Critic network model\n",
    "        self.ActorCritic = OurModel(input_shape=self.state_size, action_space=self.action_size)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.lr)\n",
    "        self.writer = tf.summary.create_file_writer(\"tensorboard\")\n",
    "\n",
    "    def act(self, state):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.ActorCritic(state, training=False)\n",
    "        action = tf.random.categorical(prediction[0], 1).numpy()\n",
    "\n",
    "        return action[0][0]\n",
    "\n",
    "    def discount_rewards(self, rewards, next_state):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(rewards)\n",
    "        for i in reversed(range(0, len(rewards))):\n",
    "            if rewards[i] != 0: # reset the sum, since this was a game boundary (pong specific!)\n",
    "                running_add = 0\n",
    "\n",
    "            running_add = running_add * gamma + rewards[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        if np.std(discounted_r) != 0.0:\n",
    "            discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "            discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
    "\n",
    "        return discounted_r\n",
    "\n",
    "    def replay(self, states, actions, rewards, next_state):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_r = self.discount_rewards(rewards, next_state)\n",
    "        discounted_r_ = np.vstack(discounted_r)\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self.ActorCritic(states, training=True)\n",
    "            action_logits = prediction[0]\n",
    "            values = prediction[1]\n",
    "\n",
    "            action_logits_selected = take_vector_elements(action_logits, actions)\n",
    "\n",
    "            advantages = discounted_r - np.stack(values)[:, 0]\n",
    "\n",
    "            action_logits_selected = tf.nn.softmax(action_logits_selected)\n",
    "            action_logits_selected_probs = tf.math.log(action_logits_selected)\n",
    "\n",
    "            action_logits_ = tf.nn.softmax(action_logits)\n",
    "            #action_logits_ = tf.math.log(action_logits_)\n",
    "            dist = tfd.Categorical(probs=action_logits_)\n",
    "            action_log_prob = dist.prob(actions)\n",
    "            action_log_prob = tf.math.log(action_log_prob)\n",
    "\n",
    "            actor_loss = -tf.math.reduce_mean(action_logits_selected_probs * advantages)\n",
    "            #actor_loss = tf.cast(actor_loss, 'float32')\n",
    "\n",
    "            action_probs = tf.nn.softmax(action_logits)\n",
    "\n",
    "            critic_loss_ = huber_loss(values, discounted_r)\n",
    "            critic_loss = mse_loss(values, discounted_r_)\n",
    "            critic_loss = tf.cast(critic_loss, 'float32')\n",
    "            \n",
    "            total_loss = actor_loss + critic_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.ActorCritic.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.ActorCritic.trainable_variables))\n",
    "\n",
    "    def load(self, model_name):\n",
    "        self.ActorCritic = load_model(model_name, compile=False)\n",
    "        #self.Critic = load_model(Critic_name, compile=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.ActorCritic.save(self.model_name)\n",
    "        #self.Critic.save(self.Model_name + '_Critic.h5')\n",
    "\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        return self.average[-1]\n",
    "\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(self.model_name + str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def GetImage(self, frame, image_memory):\n",
    "        if image_memory.shape == (1,*self.state_size):\n",
    "            image_memory = np.squeeze(image_memory)\n",
    "\n",
    "        # croping frame to 80x80 size\n",
    "        frame_cropped = frame[35:195:2, ::2,:]\n",
    "        if frame_cropped.shape[0] != self.COLS or frame_cropped.shape[1] != self.ROWS:\n",
    "            # OpenCV resize function\n",
    "            frame_cropped = cv2.resize(frame, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # converting to RGB (numpy way)\n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "\n",
    "        # convert everything to black and white (agent will train faster)\n",
    "        frame_rgb[frame_rgb < 100] = 0\n",
    "        frame_rgb[frame_rgb >= 100] = 255\n",
    "        \n",
    "        # dividing by 255 we expresses value to 0-1 representation\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "\n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        image_memory = np.roll(image_memory, 1, axis = 0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        image_memory[0,:,:] = new_frame\n",
    "\n",
    "        return np.expand_dims(image_memory, axis=0)\n",
    "\n",
    "    def reset(self):\n",
    "        image_memory = np.zeros(self.state_size)\n",
    "        obs = self.env.reset()\n",
    "        obs = np.reshape(obs, (128,128,3))\n",
    "        obs = np.array(obs).astype(np.uint8)\n",
    "        obs = cv2.resize(obs, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage(obs, image_memory)\n",
    "\n",
    "        return state\n",
    "\n",
    "    def step(self, action, image_memory):\n",
    "        next_obs, reward, done, info = self.env.step(action)\n",
    "        next_obs = np.reshape(next_obs, (128,128,3))\n",
    "        next_obs = np.array(next_obs).astype(np.uint8)\n",
    "        next_obs = cv2.resize(next_obs, dsize=(64, 64), interpolation=cv2.INTER_CUBIC)\n",
    "        next_state = self.GetImage(next_obs, image_memory)\n",
    "\n",
    "        return next_state, reward[0], done[0], info\n",
    "\n",
    "    def train(self):\n",
    "        while self.episode < self.EPISODES:\n",
    "            # Reset episode\n",
    "            score, done, SAVING = 0, False, ''\n",
    "            state = self.reset()\n",
    "\n",
    "            states, actions, rewards = [], [], []\n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, _ = self.step(action, state)\n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                score += reward\n",
    "                state = next_state\n",
    "\n",
    "            self.replay(states, actions, rewards, next_state)\n",
    "            states, actions, rewards = [], [], []\n",
    "\n",
    "            average = self.PlotModel(score, self.episode)\n",
    "            \n",
    "            # saving best models\n",
    "            if average >= self.max_average:\n",
    "                self.max_average = average\n",
    "                #self.save()\n",
    "                SAVING = \"SAVING\"\n",
    "            else:\n",
    "                SAVING = \"\"\n",
    "\n",
    "            print(\"episode: {}/{}, score: {}, average: {} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.scalar(\"average_reward\", average, step=self.episode)\n",
    "                self.writer.flush()\n",
    "            \n",
    "            if self.episode < self.EPISODES:\n",
    "                self.episode += 1\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    def test(self, Actor_name, Critic_name):\n",
    "        self.load(Actor_name, Critic_name)\n",
    "        for e in range(100):\n",
    "            state = self.reset(self.env)\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.Actor.predict(state))\n",
    "                state, reward, done, _ = self.step(action, self.env, state)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'PongDeterministic-v4'\n",
    "    #env_name = 'Pong-v0'\n",
    "    agent = A3CAgent(env_name)\n",
    "    agent.train() # use as A3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "godot_env",
   "language": "python",
   "name": "godot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
