{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzsUo6vMbPg3",
    "outputId": "7844c79a-ea20-4361-d055-9fffb99504af",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 03:16:03.390988: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created float32 vector agent_action size = 1\n",
      "Created int32 vector env_action size = 1\n",
      "Created uint32 vector observation size = 49152\n",
      "Created float32 vector reward size = 1\n",
      "Created int32 vector done size = 1\n",
      "Terminated\n",
      "Created float32 vector agent_action size = 1\n",
      "Created int32 vector env_action size = 1\n",
      "Created uint32 vector observation size = 49152\n",
      "Created float32 vector reward size = 1\n",
      "Created int32 vector done size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-04 03:16:05.403332: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-11-04 03:16:05.407097: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-11-04 03:16:05.407139: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: kimbring2-ROG-Strix-GA35DX-G35DX\n",
      "2022-11-04 03:16:05.407146: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: kimbring2-ROG-Strix-GA35DX-G35DX\n",
      "2022-11-04 03:16:05.407286: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.76.0\n",
      "2022-11-04 03:16:05.407310: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.76.0\n",
      "2022-11-04 03:16:05.407315: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 515.76.0\n",
      "2022-11-04 03:16:05.407658: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/2000000, thread: 0, score: 2.0, average: 2.00 \n",
      "episode: 1/2000000, thread: 0, score: 0.0, average: 1.00 \n",
      "episode: 2/2000000, thread: 0, score: 1.0, average: 1.00 \n",
      "episode: 3/2000000, thread: 0, score: 1.0, average: 1.00 \n",
      "episode: 4/2000000, thread: 0, score: 2.0, average: 1.20 \n",
      "episode: 5/2000000, thread: 0, score: 0.0, average: 1.00 \n",
      "episode: 6/2000000, thread: 0, score: 2.0, average: 1.14 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "import threading\n",
    "from threading import Thread, Lock\n",
    "import time\n",
    "import tensorflow_probability as tfp\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "from dodgeCreep import dodgeCreepEnv\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "#            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "  \n",
    "    \n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class OurModel(tf.keras.Model):\n",
    "    def __init__(self, input_shape, action_space):\n",
    "        super(OurModel, self).__init__()\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.dense_a = Dense(1024, activation='relu')\n",
    "        self.dense_b = Dense(1024, activation='relu')\n",
    "        self.dense_c = Dense(1024, activation='relu')\n",
    "        #self.dense_c = Dense(1024, activation='relu')\n",
    "        #self.dense_c = Dense(1024, activation='relu')\n",
    "        #self.conv_1 = Conv2D(32, 3, 2, padding=\"valid\", activation=\"relu\")\n",
    "        #self.conv_2 = Conv2D(64, 3, 2, padding=\"valid\", activation=\"relu\")\n",
    "        #self.conv_3 = Conv2D(63, 3, 2, padding=\"valid\", activation=\"relu\")\n",
    "        \n",
    "        '''\n",
    "        self.attention_1 = MultiHeadAttention(64, 4)\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout_1 = tf.keras.layers.Dropout(0.1)\n",
    "        \n",
    "        self._conv_out_size = 9\n",
    "        self._locs = []\n",
    "        for i in range(0, self._conv_out_size*self._conv_out_size):\n",
    "            self._locs.append(i / float(self._conv_out_size*self._conv_out_size))\n",
    "\n",
    "        self._locs = tf.expand_dims(self._locs, 0)\n",
    "        self._locs = tf.expand_dims(self._locs, 2)\n",
    "        '''\n",
    "            \n",
    "        self.dense_1 = Dense(action_space)\n",
    "        self.dense_2 = Dense(1)\n",
    "        \n",
    "    def call(self, X_input, training):\n",
    "        batch_size = tf.shape(X_input)[0]\n",
    "        \n",
    "        #print(\"X_input.shape: \", X_input.shape)\n",
    "        X_input = self.flatten(X_input)\n",
    "        dense_a = self.dense_a(X_input)\n",
    "        X_output = self.dense_b(dense_a)\n",
    "        #X_output = self.dense_c(dense_b)\n",
    "        #X_input = tf.transpose(X_input, perm=[0, 2, 3, 1])\n",
    "        \n",
    "        #conv_1 = self.conv_1(X_input)\n",
    "        #conv_2 = self.conv_2(conv_1)\n",
    "        #conv_3 = self.conv_3(conv_2)\n",
    "        #print(\"conv_3.shape: \", conv_3.shape)\n",
    "        \n",
    "        '''\n",
    "        conv_3_features = tf.reshape(conv_3, [batch_size,self._conv_out_size*self._conv_out_size,63])\n",
    "        \n",
    "        locs = tf.tile(self._locs, [batch_size, 1, 1])\n",
    "        conv_3_features_locs = tf.concat([conv_3_features, locs], 2)\n",
    "\n",
    "        attention_output_1, _ = self.attention_1(conv_3_features_locs, conv_3_features_locs, conv_3_features_locs, None)\n",
    "        attention_output_1 = self.dropout_1(attention_output_1, training=training)\n",
    "        attention_output_1 = self.layernorm_1(conv_3_features_locs + attention_output_1)\n",
    "\n",
    "        attention_max_pool_1d = tf.math.reduce_max(attention_output_1, 1)\n",
    "        \n",
    "        attention_flattened = Flatten()(attention_max_pool_1d)\n",
    "        \n",
    "        attention_flattened = Flatten()(dense_0)\n",
    "        '''\n",
    "        \n",
    "        action_logit = self.dense_1(X_output)\n",
    "        value = self.dense_2(X_output)\n",
    "        \n",
    "        return action_logit, value\n",
    "\n",
    "\n",
    "def safe_log(x):\n",
    "  \"\"\"Computes a safe logarithm which returns 0 if x is zero.\"\"\"\n",
    "  return tf.where(\n",
    "      tf.math.equal(x, 0),\n",
    "      tf.zeros_like(x),\n",
    "      tf.math.log(tf.math.maximum(1e-12, x)))\n",
    "\n",
    "\n",
    "def take_vector_elements(vectors, indices):\n",
    "    \"\"\"\n",
    "    For a batch of vectors, take a single vector component\n",
    "    out of each vector.\n",
    "    Args:\n",
    "      vectors: a [batch x dims] Tensor.\n",
    "      indices: an int32 Tensor with `batch` entries.\n",
    "    Returns:\n",
    "      A Tensor with `batch` entries, one for each vector.\n",
    "    \"\"\"\n",
    "    return tf.gather_nd(vectors, tf.stack([tf.range(tf.shape(vectors)[0]), indices], axis=1))\n",
    "\n",
    "\n",
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.SUM)\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "\n",
    "class A3CAgent:\n",
    "    # Actor-Critic Main Optimization Algorithm\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization\n",
    "        self.env_name = env_name       \n",
    "        GODOT_BIN_PATH = \"dodge_the_creeps/DodgeCreep.x86_64\"\n",
    "        env_abs_path = \"dodge_the_creeps/DodgeCreep.pck\"\n",
    "        self.env = dodgeCreepEnv(exec_path=GODOT_BIN_PATH, env_path=env_abs_path, turbo_mode=True)\n",
    "        \n",
    "        self.action_size = 5\n",
    "        self.EPISODES, self.episode, self.max_average = 2000000, 0, 50.0 # specific for pong\n",
    "        self.lock = Lock()\n",
    "        self.lr = 0.0001\n",
    "\n",
    "        self.ROWS = 80\n",
    "        self.COLS = 80\n",
    "        self.REM_STEP = 4\n",
    "\n",
    "        # Instantiate plot memory\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Save_Path = 'Models'\n",
    "        self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "        \n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = '{}_A3C_{}'.format(self.env_name, self.lr)\n",
    "        self.model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Create Actor-Critic network model\n",
    "        self.ActorCritic = OurModel(input_shape=self.state_size, action_space=self.action_size)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(self.lr)\n",
    "\n",
    "    def act(self, state):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.ActorCritic(state, training=False)\n",
    "        action = tf.random.categorical(prediction[0], 1).numpy()\n",
    "\n",
    "        return action[0][0]\n",
    "\n",
    "    def discount_rewards(self, reward, dones):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0, len(reward))):\n",
    "            if reward[i] != 0: # reset the sum, since this was a game boundary (pong specific!)\n",
    "                running_add = 0\n",
    "\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        if np.std(discounted_r) != 0:\n",
    "            discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "            discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
    "\n",
    "        return discounted_r\n",
    "        \n",
    "    def replay(self, states, actions, rewards, dones):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        \n",
    "        # Compute discounted rewards\n",
    "        discounted_r = self.discount_rewards(rewards, dones)\n",
    "        discounted_r_ = np.vstack(discounted_r)\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self.ActorCritic(states, training=True)\n",
    "            action_logits = prediction[0]\n",
    "            values = prediction[1]\n",
    "            \n",
    "            action_logits_selected = take_vector_elements(action_logits, actions)\n",
    "            \n",
    "            advantages = discounted_r - np.stack(values)[:, 0] \n",
    "            \n",
    "            action_logits_selected = tf.nn.softmax(action_logits_selected)\n",
    "            action_logits_selected_probs = tf.math.log(action_logits_selected)\n",
    "            \n",
    "            action_logits_ = tf.nn.softmax(action_logits)\n",
    "            dist = tfd.Categorical(probs=action_logits_)\n",
    "            action_log_prob = dist.prob(actions)\n",
    "            action_log_prob = tf.math.log(action_log_prob)\n",
    "            \n",
    "            actor_loss = -tf.math.reduce_mean(action_logits_selected_probs * advantages) \n",
    "            \n",
    "            action_probs = tf.nn.softmax(action_logits)\n",
    "            \n",
    "            #critic_loss = huber_loss(values, discounted_r)\n",
    "            critic_loss = mse_loss(values, discounted_r)\n",
    "            critic_loss = tf.cast(critic_loss, 'float32')\n",
    "            #print(\"critic_loss: \", critic_loss)\n",
    "            \n",
    "            entropy_loss = -tf.math.reduce_mean(action_logits_selected * tf.math.log(action_logits_selected))\n",
    "            \n",
    "            total_loss = actor_loss + critic_loss\n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.ActorCritic.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.ActorCritic.trainable_variables))\n",
    "        \n",
    "    def load(self, model_name):\n",
    "        self.ActorCritic = load_model(model_name, compile=False)\n",
    "        #self.Critic = load_model(Critic_name, compile=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.ActorCritic.save(self.model_name)\n",
    "        #self.Critic.save(self.Model_name + '_Critic.h5')\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            pylab.plot(self.episodes, self.scores, 'b')\n",
    "            pylab.plot(self.episodes, self.average, 'r')\n",
    "            pylab.ylabel('Score', fontsize=18)\n",
    "            pylab.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                pylab.savefig(self.path + \".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "    \n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(self.model_name + str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def GetImage(self, frame, image_memory):\n",
    "        if image_memory.shape == (1,*self.state_size):\n",
    "            image_memory = np.squeeze(image_memory)\n",
    "        \n",
    "        # croping frame to 80x80 size\n",
    "        frame_cropped = frame[35:195:2, ::2,:]\n",
    "        if frame_cropped.shape[0] != self.COLS or frame_cropped.shape[1] != self.ROWS:\n",
    "            # OpenCV resize function \n",
    "            frame_cropped = cv2.resize(frame, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "        \n",
    "        #print(\"frame_cropped.shape: \", frame_cropped.shape)\n",
    "        \n",
    "        # converting to RGB (numpy way)\n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "        #frame_rgb = frame_cropped\n",
    "        \n",
    "        #print(\"frame_rgb: \", frame_rgb)\n",
    "        \n",
    "        # convert everything to black and white (agent will train faster)\n",
    "        frame_rgb[frame_rgb < 100] = 0\n",
    "        frame_rgb[frame_rgb >= 150] = 255\n",
    "        \n",
    "        # dividing by 255 we expresses value to 0-1 representation\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "\n",
    "        cv2.imshow(\"new_frame: \", new_frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        image_memory = np.roll(image_memory, 1, axis=0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        image_memory[0,:,:] = new_frame\n",
    "\n",
    "        return np.expand_dims(image_memory, axis=0)\n",
    "\n",
    "    def reset(self, env):\n",
    "        image_memory = np.zeros(self.state_size)\n",
    "        frame = env.reset()\n",
    "        frame = np.reshape(frame, (128,128,3))\n",
    "        frame = frame.astype(np.uint8)\n",
    "        #frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage(frame, image_memory)\n",
    "            \n",
    "        return state\n",
    "    \n",
    "    def step(self, action, env, image_memory):\n",
    "        next_frame, reward, done, info = env.step(action)\n",
    "        next_frame = np.reshape(next_frame, (128,128,3))\n",
    "        next_frame = next_frame.astype(np.uint8)\n",
    "        #next_frame = cv2.cvtColor(next_frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        next_state = self.GetImage(next_frame, image_memory)\n",
    "        \n",
    "        return next_state, reward, done, info\n",
    "    \n",
    "    def train(self, n_threads):\n",
    "        self.env.close()\n",
    "        # Instantiate one environment per thread\n",
    "        GODOT_BIN_PATH = \"dodge_the_creeps/DodgeCreep.x86_64\"\n",
    "        env_abs_path = \"dodge_the_creeps/DodgeCreep.pck\"\n",
    "        envs = [dodgeCreepEnv(exec_path=GODOT_BIN_PATH, env_path=env_abs_path, turbo_mode=True) \n",
    "                for i in range(n_threads)]\n",
    "\n",
    "        # Create threads\n",
    "        threads = [threading.Thread(target=self.train_threading, daemon=True, \n",
    "                                    args=(self, envs[i], i)) for i in range(n_threads)]\n",
    "\n",
    "        for t in threads:\n",
    "            time.sleep(1)\n",
    "            t.start()\n",
    "            \n",
    "        for t in threads:\n",
    "            time.sleep(1)\n",
    "            t.join()\n",
    "            \n",
    "    def train_threading(self, agent, env, thread):\n",
    "        while self.episode < self.EPISODES:\n",
    "            # Reset episode\n",
    "            score, done, SAVING = 0, False, ''\n",
    "            \n",
    "            state = self.reset(env)\n",
    "\n",
    "            states, actions, rewards, dones = [], [], [], []\n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, _ = self.step(action, env, state)\n",
    "                \n",
    "                #print(\"action: \", action)\n",
    "                #print(\"next_state.shape: \", next_state.shape)\n",
    "                \n",
    "                reward = reward[0]\n",
    "                done = done[0]\n",
    "                \n",
    "                #print(\"reward: \", reward)\n",
    "                #print(\"done: \", done)\n",
    "                #next_state = np.expand_dims(next_state, axis=0)\n",
    "                    \n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "\n",
    "                score += reward\n",
    "                state = next_state\n",
    "            \n",
    "            try:\n",
    "                self.lock.acquire()\n",
    "                self.replay(states, actions, rewards, dones)\n",
    "                self.lock.release()\n",
    "            except:\n",
    "                print(\"except\")\n",
    "                print(\"states: \", states)\n",
    "                print(\"actions: \", actions)\n",
    "                print(\"rewards: \", rewards)\n",
    "                print(\"dones: \", dones)\n",
    "                pass\n",
    "            \n",
    "            states, actions, rewards, dones = [], [], [], []\n",
    "                    \n",
    "            # Update episode count\n",
    "            with self.lock:\n",
    "                average = self.PlotModel(score, self.episode)\n",
    "                # saving best models\n",
    "                if average >= self.max_average:\n",
    "                    self.max_average = average\n",
    "                    #self.save()\n",
    "                    SAVING = \"SAVING\"\n",
    "                else:\n",
    "                    SAVING = \"\"\n",
    "\n",
    "                print(\"episode: {}/{}, thread: {}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, thread, score, average, SAVING))\n",
    "                if(self.episode < self.EPISODES):\n",
    "                    self.episode += 1\n",
    "\n",
    "        env.close()            \n",
    "\n",
    "    def test(self, Actor_name, Critic_name):\n",
    "        self.load(Actor_name, Critic_name)\n",
    "        for e in range(100):\n",
    "            state = self.reset(self.env)\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.Actor.predict(state))\n",
    "                state, reward, done, _ = self.step(action, self.env, state)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'DodgeCreep'\n",
    "    agent = A3CAgent(env_name)\n",
    "    \n",
    "    #agent.run() # use as A2C\n",
    "    agent.train(n_threads=1) # use as A3C\n",
    "    #agent.test('Models/Pong-v0_A3C_2.5e-05_Actor.h5', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pong-v0_A3C_TF2(Working).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
